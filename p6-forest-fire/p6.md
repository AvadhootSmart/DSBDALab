Below is the Python code to integrate Python with Hadoop and perform the specified operations on the Forest Fire dataset using MapReduce in PyHadoop and data mining in Hive. The Forest Fire dataset (available from the UCI Machine Learning Repository) contains attributes like temperature, wind, and area burned, suitable for big data analysis. Since Hadoop setups can be complex and may not be feasible to set up in one day, I'll provide detailed code with clear instructions, leveraging Hadoop Streaming for MapReduce (as PyHadoop is less commonly used) and PyHive for Hive integration. I'll also include a viva cheatsheet summarizing key concepts for your exam.
Python Code for Forest Fire Dataset Operations
Prerequisites

    Dataset: UCI Forest Fire dataset (forestfires.csv from UCI Machine Learning Repository).
    Hadoop Setup: Hadoop installed and running (HDFS and YARN). If not available, use a Hadoop cluster in a lab or cloud environment (e.g., AWS EMR).
    Python Libraries: Install required libraries:
    bash

    pip install pandas pyhive thrift

    Hadoop Streaming JAR: Ensure the Hadoop Streaming JAR is available (e.g., /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming.jar).
    Hive: Hive server running with access to HDFS.

Directory Setup

    Copy the dataset to HDFS:
    bash

    hdfs dfs -mkdir /user/forestfires
    hdfs dfs -put forestfires.csv /user/forestfires/

Code
python

import pandas as pd
import sys
import os
from pyhive import hive

# Part a: Data Analysis using MapReduce in PyHadoop (using Hadoop Streaming)
# Example: Count fires by month using MapReduce

# Mapper script (mapper.py)
mapper_code = """
#!/usr/bin/env python
import sys

# Read input from standard input
for line in sys.stdin:
    line = line.strip()
    # Skip header
    if line.startswith('X,Y,month'):
        continue
    # Split CSV line
    fields = line.split(',')
    if len(fields) >= 3:
        month = fields[2]  # Month is the third column
        # Emit month and count
        print(f'{month}\t1')
"""

# Reducer script (reducer.py)
reducer_code = """
#!/usr/bin/env python
import sys

current_month = None
current_count = 0

# Read input from standard input
for line in sys.stdin:
    line = line.strip()
    month, count = line.split('\t')
    count = int(count)
    
    # If same month, increment count
    if current_month == month:
        current_count += count
    else:
        # Output previous month count
        if current_month:
            print(f'{current_month}\t{current_count}')
        current_month = month
        current_count = count

# Output the last month
if current_month:
    print(f'{current_month}\t{current_count}')
"""

# Save mapper and reducer scripts
with open('mapper.py', 'w') as f:
    f.write(mapper_code)
with open('reducer.py', 'w') as f:
    f.write(reducer_code)

# Make scripts executable
os.system('chmod +x mapper.py reducer.py')

# Run Hadoop Streaming job
hadoop_cmd = """
hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming.jar \
    -input /user/forestfires/forestfires.csv \
    -output /user/forestfires/output \
    -mapper mapper.py \
    -reducer reducer.py \
    -file mapper.py \
    -file reducer.py
"""

print("Running MapReduce job...")
os.system(hadoop_cmd)

# Display results from HDFS
print("MapReduce Results (Fires by Month):")
os.system('hdfs dfs -cat /user/forestfires/output/part-*')

# Part b: Data Mining in Hive
# Connect to Hive and perform queries

# Establish Hive connection
conn = hive.connect(host='localhost', port=10000, database='default')
cursor = conn.cursor()

# Create a Hive table for the Forest Fire dataset
create_table_query = """
CREATE TABLE IF NOT EXISTS forest_fires (
    X INT,
    Y INT,
    month STRING,
    day STRING,
    FFMC FLOAT,
    DMC FLOAT,
    DC FLOAT,
    ISI FLOAT,
    temp FLOAT,
    RH FLOAT,
    wind FLOAT,
    rain FLOAT,
    area FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
TBLPROPERTIES ('skip.header.line.count'='1')
"""

cursor.execute(create_table_query)

# Load data from HDFS into Hive table
load_data_query = """
LOAD DATA INPATH '/user/forestfires/forestfires.csv' 
OVERWRITE INTO TABLE forest_fires
"""

cursor.execute(load_data_query)

# Data Mining Query 1: Average area burned by month
query1 = """
SELECT month, AVG(area) as avg_area_burned
FROM forest_fires
GROUP BY month
"""

cursor.execute(query1)
results = cursor.fetchall()
print("\nHive Query 1: Average Area Burned by Month")
for row in results:
    print(row)

# Data Mining Query 2: Top 5 conditions with highest fire area
query2 = """
SELECT temp, wind, area
FROM forest_fires
WHERE area > 0
ORDER BY area DESC
LIMIT 5
"""

cursor.execute(query2)
results = cursor.fetchall()
print("\nHive Query 2: Top 5 Conditions with Highest Fire Area")
for row in results:
    print(row)

# Close Hive connection
cursor.close()
conn.close()

Viva Cheatsheet for Assignment 3
Dataset Overview

    Forest Fire Dataset: UCI dataset with attributes like month, temp, wind, area (burned area in hectares).
    Source: UCI Machine Learning Repository (search “Forest Fires UCI”).
    Format: CSV with 13 columns, including spatial (X, Y), temporal (month, day), and environmental (temp, wind, area) data.
    Key Columns:
        month: Month of fire (e.g., jan, feb).
        temp: Temperature (°C).
        wind: Wind speed (km/h).
        area: Burned area (hectares).

Key Tools

    Hadoop: Framework for distributed storage (HDFS) and processing (MapReduce/YARN).
    Hadoop Streaming: Allows Python scripts for MapReduce jobs using standard input/output.
    Hive: SQL-like interface for querying data in HDFS.
    Python Libraries:
        PyHive: Connects Python to Hive (pip install pyhive thrift).
        Pandas: Optional for local data inspection.
    Install:
    bash

    pip install pandas pyhive thrift

Operations and Concepts

    Data Analysis using MapReduce in PyHadoop (Hadoop Streaming)
        What: Process large datasets in parallel using MapReduce.
        Why: Efficiently analyze big data (e.g., count fires by month).
        Components:
            Mapper: Processes input data, emits key-value pairs (e.g., (month, 1)).
            Reducer: Aggregates mapper output (e.g., sums counts for each month).
            Hadoop Streaming: Uses Python scripts with stdin/stdout for MapReduce.
        Code:
            Mapper: Read CSV, emit (month, 1) for each row.
            python

            for line in sys.stdin:
                fields = line.split(',')
                month = fields[2]
                print(f'{month}\t1')

            Reducer: Sum counts for each month.
            python

            current_month = None
            current_count = 0
            for line in sys.stdin:
                month, count = line.split('\t')
                if current_month == month:
                    current_count += count
                else:
                    if current_month:
                        print(f'{current_month}\t{current_count}')
                    current_month = month
                    current_count = count

            Run job:
            bash

            hadoop jar hadoop-streaming.jar -input /path -output /path -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py

        Example: Count fires by month to identify high-risk periods.
        Tip: Skip header rows in mapper; ensure scripts are executable (chmod +x).
    Data Mining in Hive
        What: Query HDFS data using SQL-like syntax (HiveQL).
        Why: Extract insights (e.g., average area burned, high-risk conditions).
        Steps:
            Create table: Define schema matching dataset.
            sql

            CREATE TABLE forest_fires (
                month STRING,
                temp FLOAT,
                area FLOAT
            )
            ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
            TBLPROPERTIES ('skip.header.line.count'='1');

            Load data: Move CSV from HDFS to Hive table.
            sql

            LOAD DATA INPATH '/user/forestfires/forestfires.csv' INTO TABLE forest_fires;

            Query data: Use HiveQL for analysis.
            sql

            SELECT month, AVG(area) FROM forest_fires GROUP BY month;

        Python Integration:
            Use pyhive to connect and execute queries.
            python

            from pyhive import hive
            conn = hive.connect(host='localhost', port=10000)
            cursor = conn.cursor()
            cursor.execute("SELECT month, AVG(area) FROM forest_fires GROUP BY month")
            results = cursor.fetchall()

        Example Queries:
            Average area burned by month: Identify seasonal patterns.
            Top conditions: Find temperature/wind conditions for large fires.
        Tip: Ensure Hive server is running; handle header rows with TBLPROPERTIES.

General Tips

    Hadoop Setup:
        Start HDFS and YARN:
        bash

        start-dfs.sh
        start-yarn.sh

        Verify HDFS: hdfs dfs -ls /
        Check Hive: hive --service hiveserver2
    Dataset Prep:
        Upload to HDFS: hdfs dfs -put forestfires.csv /user/forestfires/
        Verify: hdfs dfs -ls /user/forestfires/
    Debugging:
        MapReduce errors: Check YARN logs via yarn logs -applicationId <app_id>.
        Hive errors: Ensure table schema matches CSV; check Hive server connection.
    Practice: If no Hadoop setup, simulate MapReduce locally by piping data (cat forestfires.csv | python mapper.py | sort | python reducer.py). Study Hive queries.
    Exam Focus:
        Explain MapReduce phases (map, shuffle, reduce).
        Describe Hive’s role in data warehousing.
        Be ready to write mapper/reducer logic or simple Hive queries.

Quick Cheatsheet
Operation
	
Tool/Code
	
Example Usage
Upload to HDFS
	
hdfs dfs -put file.csv /path
	
hdfs dfs -put forestfires.csv /user/forestfires/
Mapper (Python)
	
for line in sys.stdin: print(key\tvalue)
	
print(f'{month}\t1')
Reducer (Python)
	
Aggregate key-value pairs
	
print(f'{month}\t{current_count}')
Run MapReduce
	
hadoop jar hadoop-streaming.jar ...
	
hadoop jar hadoop-streaming.jar -mapper mapper.py
Create Hive Table
	
CREATE TABLE ... ROW FORMAT DELIMITED
	
CREATE TABLE forest_fires (month STRING, ...)
Load Data (Hive)
	
LOAD DATA INPATH ...
	
LOAD DATA INPATH '/user/forestfires/forestfires.csv'
Hive Query
	
SELECT ... FROM ...
	
SELECT month, AVG(area) FROM forest_fires GROUP BY month
Connect Hive (Python)
	
hive.connect(host='localhost')
	
conn = hive.connect(host='localhost', port=10000)
How to Use

    Get the Dataset: Download the Forest Fire dataset from UCI Machine Learning Repository (search “Forest Fires UCI”). Save as forestfires.csv.
    Hadoop Environment:
        Ensure Hadoop and Hive are running.
        Upload dataset to HDFS: hdfs dfs -put forestfires.csv /user/forestfires/.
    Run the Code:
        Save the Python script and run it in a Hadoop-enabled environment.
        For MapReduce, ensure hadoop-streaming.jar path is correct.
        For Hive, verify Hive server is running (localhost:10000 by default).
    Practice:
        Modify MapReduce to analyze another column (e.g., count fires by day).
        Try additional Hive queries (e.g., SELECT temp, COUNT(*) FROM forest_fires WHERE area > 0 GROUP BY temp).
    Exam Prep: Use the cheatsheet to recall commands and explain concepts. Be ready to describe MapReduce flow or write a Hive query.

Notes for Execution

    Hadoop Streaming: Used instead of PyHadoop due to wider adoption and simplicity for Python integration. It allows Python scripts to act as mappers/reducers via stdin/stdout.
    PyHive: Simplifies Hive integration with Python, ideal for executing SQL-like queries.
    If No Hadoop Setup: Focus on understanding the code logic and viva cheatsheet. Simulate MapReduce locally (see “Practice” tip above).

If you face issues (e.g., Hadoop setup, Hive connection, or dataset access), let me know, and I can provide workarounds or further clarification. Good luck with your exam!
