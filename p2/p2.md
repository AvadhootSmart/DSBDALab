Perfect! üöÄ  
Here‚Äôs a **ready-to-use folder structure** + basic setup instructions you can quickly follow for your **practical exam**.

---

# üìÅ Folder Structure for MapReduce Log Processing Project

```
LogProcessor/
‚îú‚îÄ‚îÄ build.xml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ LogDriver.java
‚îÇ   ‚îú‚îÄ‚îÄ LogMapper.java
‚îÇ   ‚îî‚îÄ‚îÄ LogReducer.java
```

---

# üß© File Contents

---

## `LogMapper.java`

```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class LogMapper extends Mapper<Object, Text, Text, IntWritable> {

    private Text username = new Text();
    private IntWritable sessionDuration = new IntWritable();

    @Override
    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        String[] parts = value.toString().split("\\s+");
        if (parts.length == 2) {
            username.set(parts[0]);
            sessionDuration.set(Integer.parseInt(parts[1]));
            context.write(username, sessionDuration);
        }
    }
}
```

---

## `LogReducer.java`

```java
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class LogReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int totalDuration = 0;
        for (IntWritable val : values) {
            totalDuration += val.get();
        }
        context.write(key, new IntWritable(totalDuration));
    }
}
```

---

## `LogDriver.java`

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class LogDriver {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "User Login Time Calculation");

        job.setJarByClass(LogDriver.class);
        job.setMapperClass(LogMapper.class);
        job.setReducerClass(LogReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));   // Input path
        FileOutputFormat.setOutputPath(job, new Path(args[1])); // Output path

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

---

## `build.xml` (for Apache Ant build)

```xml
<project name="LogProcessor" default="jar" basedir=".">
    <property name="src" location="src"/>
    <property name="build" location="build"/>
    <property name="dist" location="dist"/>
    
    <target name="init">
        <mkdir dir="${build}"/>
        <mkdir dir="${dist}"/>
    </target>

    <target name="compile" depends="init">
        <javac srcdir="${src}" destdir="${build}" includeantruntime="false">
            <classpath>
                <pathelement path="/usr/local/hadoop/share/hadoop/common/hadoop-common-*.jar"/>
                <pathelement path="/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-*.jar"/>
            </classpath>
        </javac>
    </target>

    <target name="jar" depends="compile">
        <jar destfile="${dist}/LogProcessor.jar" basedir="${build}"/>
    </target>

    <target name="clean">
        <delete dir="${build}"/>
        <delete dir="${dist}"/>
    </target>
</project>
```

> **Note:** Adjust Hadoop jar paths if your Hadoop is installed somewhere else (like `/opt/hadoop/`).

---

# üî• Steps to Compile and Run

### 1. Create Folder
```bash
mkdir LogProcessor
cd LogProcessor
```

### 2. Add Files
- Create the folders: `src/`
- Add the 3 Java files inside `src/`
- Add `build.xml` at root.

---

### 3. Compile Project
```bash
ant jar
```
‚úÖ This will create `dist/LogProcessor.jar`.

---

### 4. Upload Input Data to HDFS
```bash
hdfs dfs -mkdir /input
hdfs dfs -put system_log.txt /input/
```

*(Make sure `system_log.txt` contains your sample logs.)*

---

### 5. Run the MapReduce Job
```bash
hadoop jar dist/LogProcessor.jar LogDriver /input /output
```

---

### 6. See the Output
```bash
hdfs dfs -cat /output/part-00000
```

---

# ‚úçÔ∏è In Practical Exam, Mention:

- ‚ÄúI built a MapReduce project with **separate Mapper, Reducer, and Driver classes**."
- ‚ÄúCompiled using **Apache Ant**."
- ‚ÄúExecuted in **pseudo-distributed mode** on **Hadoop**."
- "Used **HDFS** for storing input and output files."

---

# ‚úÖ That's your full practical-ready project!

---

Would you also want me to give you a **sample system_log.txt** example that you can immediately upload and use? üìÑ  
(will save you time tomorrow) üöÄ‚ú®  
Should I make a quick one?
